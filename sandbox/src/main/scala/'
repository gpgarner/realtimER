package com.sandbox.src.main.scala

import org.apache.spark.SparkContext
import org.apache.spark.SparkContext._
import org.apache.spark.SparkConf
import org.apache.spark.sql.functions._

import java.util.concurrent.TimeUnit

import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.streaming.{OutputMode, Trigger}
import org.apache.spark.sql.types._//{StringType, StructField, StructType, DateType}
import org.apache.commons.lang.StringUtils.substring
import java.text.SimpleDateFormat

import java.util.Properties
//import kafkashaded.org.apache.kafka.clients.producer._
import org.apache.spark.sql.ForeachWriter
import org.apache.kafka.clients.producer._
import org.apache.kafka.common._
import org.apache.spark.sql.streaming.ProcessingTime
import org.apache.spark.ml.feature.NGram
import java.sql.{Array=>SQLArray,_}

import org.apache.spark.ml.feature._
import org.apache.spark.ml.linalg._
import org.apache.spark.sql.types._

import scala.collection.mutable

object FileStreamExample {

  def main(args: Array[String]): Unit = {

    val sparkSession = SparkSession.builder
      .master("spark://18.205.181.166:7077")
      .appName("example")
      .getOrCreate()

    import sparkSession.implicits._
    
    /////////////////////////////////////////////////////////////////////////////////////////////////////////    
 
    class  JDBCSink(url:String, user:String, pwd:String, dbase: String) extends org.apache.spark.sql.ForeachWriter[org.apache.spark.sql.Row] {
      val driver = "org.postgresql.Driver"
      var connection:Connection = _
      var statement:Statement = _
    
      def open(partitionId: Long,version: Long): Boolean = {
        Class.forName(driver)
        connection =java.sql.DriverManager.getConnection(url, user, pwd)
        statement = connection.createStatement
        true
      }

      def process(value: org.apache.spark.sql.Row): Unit = {
        statement.executeUpdate("INSERT INTO public." + dbase +"(name,city_name,state_init,zip_code) " + "VALUES ('" + value(0) + "','" + value(1) + "','" + value(2) + "','" + value(3) +  "');")
      }

      def close(errorOrNull: Throwable): Unit = {
        connection.close()
      }
    } 
    
    val schema = StructType(
      Array(StructField("CMTE_ID", StringType),
                StructField("AMNDT_IND", StringType),
                StructField("RPT_TP", StringType),
                StructField("TRANSACTION_PGI", StringType),
                StructField("IMAGE_NUM", StringType),
                StructField("TRANSACTION_TP", StringType),
                StructField("ENTITY_TP", StringType),
                StructField("NAME", StringType),
                StructField("CITY", StringType),
                StructField("STATE", StringType),
                StructField("ZIP_CODE", StringType),
                StructField("EMPLOYER", StringType),
                StructField("OCCUPATION", StringType),
                StructField("TRANSACTION_DT", StringType),
                StructField("TRANSACTION_AMT", StringType),
                StructField("OTHER_ID", StringType),
                StructField("TRAN_ID", StringType),
                StructField("FILE_NUM", StringType),
                StructField("MEMO_CD", StringType),
                StructField("MEMO_TEXT", StringType),
		StructField("SUB_ID", StringType)))

    /////////////////////////////////////////////////////////////////////////////////////////////////    
 
    val dfCity = sparkSession.read
         .format("csv")
         .option("header", "true") //reading the headers
         .option("mode", "DROPMALFORMED")
         .load("hdfs://ec2-18-205-181-166.compute-1.amazonaws.com:9000/user/uscitiesv14.csv")
    val dfCityUpper = dfCity.withColumn("city",upper($"city"))

    val dfCityUpperConcat = dfCityUpper.withColumn("cityState", concat($"city",lit(" "),$"state_id"))
    
    val expectedNumItems: Long = 10000 
    val fpp: Double = 0.005

    val sbf = dfCityUpperConcat.stat.bloomFilter($"cityState", expectedNumItems, fpp)

    /////////////////////////////////////////////////////////////////////////////////////////////////

    val dfGroundTruth = sparkSession.read
         .format("csv")
         .option("header", "true") //reading the headers
         .option("mode", "DROPMALFORMED")
         .load("hdfs://ec2-18-205-181-166.compute-1.amazonaws.com:9000/user/base_unique4.csv")

    val dfGroundTruthMod = dfGroundTruth.select("NAME","CITY","STATE","ZIP_CODE")
 
    dfGroundTruthMod.show()
    dfGroundTruthMod.printSchema
   
    def splitFunc: (String => Array[String]) = {s => s.split("")}
    val mySplitFunc = udf(splitFunc)
    def stringFunc: (mutable.WrappedArray[String] => String) = {s => s.mkString("").replace("   "," ").replace("  ", " ")}
    val myStringFunc = udf(stringFunc)

    val ngrammable = dfGroundTruthMod
        .withColumn("NGRAM_NAME",mySplitFunc($"NAME"))

    val ngram = new NGram().setN(3).setInputCol("NGRAM_NAME").setOutputCol("ngrams")
    val ngramDataFrame = ngram.transform(ngrammable)
    
    val ngramDataFrameStr = ngramDataFrame.withColumn("ngramString", myStringFunc($"ngrams"))
    
    ngramDataFrameStr.select("ngramString").show()
    
    val tokenizer = new Tokenizer().setInputCol("ngramString").setOutputCol("grams")
    val gramsDf = tokenizer.transform(ngramDataFrameStr)
    
    val vocabSize = 1000000
    val cvModel: CountVectorizerModel = new CountVectorizer().setInputCol("grams").setOutputCol("features").setVocabSize(vocabSize).setMinDF(10).fit(gramsDf)
    val isNoneZeroVector = udf({v: Vector => v.numNonzeros > 0}, DataTypes.BooleanType)

    val vectorizedDf = cvModel.transform(gramsDf).filter(isNoneZeroVector(col("features"))).select(col("NAME"), col("features"))
    vectorizedDf.show()

    val mh = new MinHashLSH().setNumHashTables(3).setInputCol("features").setOutputCol("hashValues")
    val model = mh.fit(vectorizedDf)
    
    model.transform(vectorizedDf).show()
    
    val threshold = 0.2
    //model.approxSimilarityJoin(vectorizedDf, vectorizedDf, threshold).filter("distCol != 0").show()

    //val dfHashed = dfGroundTruthMod.withColumn("hash", hash(dfGroundTruthMod.columns.map(col): _*))    
    //dfHashed.show()
    //dfNGram.show()

    //////////////////////////////////////////////////////////////////////////////////////////////////

    //create stream from kafka topic data
    val fileStreamDf = sparkSession
	.readStream
	.format("kafka")
 	.option("kafka.bootstrap.servers", "18.205.181.166:9092")
 	.option("subscribe", "data")
        .option("startingOffsets","earliest")
        .option("maxOffsetsPerTrigger",100)
 	.load()
    
    val df_string = fileStreamDf.selectExpr("CAST(value AS STRING)")
    
    var df = df_string.select(from_json(col("value"),schema).alias("data")).select("data.*")

    /*val df = sparkSession
        .read.format("csv")
        .option("header","true")
        .option("delimiter", ",")
        .schema(schema)
        .load("hdfs://ec2-18-205-181-166.compute-1.amazonaws.com:9000/user/base_unique.csv")
    df.show()*/
    //////////////////////////////////////////////////////////////////////////////////////////////////

    def cityExistFunc: (String => Boolean) = (s => sbf.mightContain(s))
    def cityNoExistFunc: (String => Boolean) = (s => !sbf.mightContain(s))
    val myCityNoExistFunc = udf(cityNoExistFunc)
    val myCityExistFunc = udf(cityExistFunc)

    def dateFunc: (String => String) = {s => substring(s,4)}
    val myDateFunc = udf(dateFunc)
    def zipFunc: (String => String) = {s => substring(s,0,5)}
    val myZipFunc = udf(zipFunc)
    def nameFunc: (String => String) = {s => s.replace("'","''")}
    val myNameFunc = udf(nameFunc)
    def cityFunc: (String => String) = {s => s.replace("'","''").replace("ST. ", "SAINT ") }
    val myCityFunc = udf(cityFunc)

    //////////////////////////////////////////////////////////////////////////////////////////////////
    
   val dfFilter0 = df
      .filter(col("CMTE_ID")!=="")
      .filter(col("NAME")!=="")
      .filter(col("ZIP_CODE")!=="")
      .filter(col("TRANSACTION_DT")!=="")
      .filter(col("TRANSACTION_AMT")!=="")
      .filter(col("OTHER_ID")==="")
    /*val dfFilter0 = df
      .filter(col("CMTE_ID").isNotNull())
      .filter(col("NAME").isNotNull())
      .filter(col("ZIP_CODE").isNotNull())
      .filter(col("TRANSACTION_DT").isNotNull())
      .filter(col("TRANSACTION_AMT").isNotNull())
      .filter(col("OTHER_ID").isNull())*/

    val dfAlter1 = dfFilter0
      .withColumn("ZIP_CODE", myZipFunc(dfFilter0("ZIP_CODE")))
      .withColumn("YEAR", myDateFunc(dfFilter0("TRANSACTION_DT")).cast(IntegerType))
      .withColumn("TRANSACTION_AMT",dfFilter0("TRANSACTION_AMT").cast(IntegerType))
      .withColumn("NAME",myNameFunc(dfFilter0("NAME")))
      .withColumn("CITY",myCityFunc(dfFilter0("CITY")))

   val dfAlter2 = dfAlter1
      .filter(col("YEAR")<=2018)
      .filter(col("YEAR")>=1980)

   val dfAlter3 = dfAlter2
      .withColumn("timestamp",to_timestamp($"TRANSACTION_DT", "MMddyyyy"))
      .withColumn("TRANSACTION_DT",to_date($"TRANSACTION_DT", "MMddyyyy").alias("date"))
      .withColumn("CITYSTATE", concat($"CITY", lit(" "), $"STATE"))
    //val dfCityNoExists = dfAlter3
    //  .filter(myCityNoExistFunc($"CITYSTATE"))
    //val dfCityExists = dfAlter3
    //  .filter(myCityExistFunc($"CITYSTATE"))
    ///////////////////////////////////////////////////////////////////////////////////////////////////
    val ngrammable0 = dfAlter3
        .withColumn("NGRAM_NAME",mySplitFunc($"NAME"))
    val ngram0 = new NGram().setN(3).setInputCol("NGRAM_NAME").setOutputCol("ngrams")
    val ngramDataFrame0 = ngram.transform(ngrammable0)
    val ngramDataFrameStr0 = ngramDataFrame0.withColumn("ngramString", myStringFunc($"ngrams"))
    val tokenizer0 = new Tokenizer().setInputCol("ngramString").setOutputCol("grams")
    val gramsDf0 = tokenizer0.transform(ngramDataFrameStr0)

    val vectorizedDf0 = cvModel.transform(gramsDf0).filter(isNoneZeroVector(col("features"))).select(col("NAME"), col("features"))

    //val mh0 = new MinHashLSH().setNumHashTables(3).setInputCol("features").setOutputCol("hashValues")


    //model.approxSimilarityJoin(vectorizedDf, vectorizedDf0, threshold).filter("distCol != 0").show()

    
    ////////////////////////////////////////////////////////////////////////////////////////////////////

    val url="jdbc:postgresql://postgresgpgsql.c0npzf7zoofq.us-east-1.rds.amazonaws.com:5432/postgresMVP"
    val user="gpgarner8324"
    val pwd="carmenniove84!"
    val writerNo = new JDBCSink(url, user, pwd, "citynoexist")
    val writerYes = new JDBCSink(url, user, pwd, "cityexist")
    val vectorizedQUERY = model.approxSimilarityJoin(vectorizedDf0, vectorizedDf, threshold).filter("distCol != 0")

    val query = vectorizedQUERY
      .writeStream
      .format("console")
      .trigger(ProcessingTime("25 seconds"))
      .start()
    query.awaitTermination()

    /*val query0 = dfCityNoExists.select("NAME","CITY","STATE","ZIP_CODE")
      .writeStream
      .foreach(writerNo)
      .outputMode("update")
      .trigger(ProcessingTime("25 seconds"))
      .start()

    val query1 = dfCityExists.select("NAME","CITY","STATE","ZIP_CODE")
      .writeStream
      .foreach(writerYes)
      .outputMode("update")
      .trigger(ProcessingTime("25 seconds"))
      .start()*/
      
    //query0.awaitTermination()
  
  }
}
